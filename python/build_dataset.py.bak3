import os
import argparse
from pathlib import Path
from typing import Tuple

import pandas as pd


def _require_venv() -> None:
    """
    Keep the same safety pattern as run_engine.py: require an activated venv.
    """
    if os.environ.get("VIRTUAL_ENV") is None:
        raise RuntimeError(
            "Virtual environment is not activated. "
            "Activate your venv before running the dataset builder."
        )


def _find_header_row(raw: pd.DataFrame, marker: str = "SELLER Loan #", max_scan_rows: int = 40) -> int:
    """
    Exhibit A sheets often contain title rows above the real header.
    Heuristic: find the first row that contains the header marker.
    """
    scan = raw.head(max_scan_rows)
    for i in range(len(scan)):
        row = scan.iloc[i].astype(str).fillna("")
        if (row == marker).any():
            return i
    raise ValueError(f"Could not find header row containing marker '{marker}' in first {max_scan_rows} rows.")


def _clean_exhibit_a(excel_path: str, platform: str) -> pd.DataFrame:
    """
    Exhibit A ingestion + cleaning:
    - read excel with header=None
    - detect header row by marker 'SELLER Loan #'
    - set columns from header row
    - drop empty rows
    - drop trailer rows (where SELLER Loan # is null)
    - add Platform column ('prime' / 'sfy')
    """
    raw = pd.read_excel(excel_path, header=None)
    header_row = _find_header_row(raw, marker="SELLER Loan #")

    headers = raw.iloc[header_row].tolist()
    df = raw.iloc[header_row + 1 :].copy()
    df.columns = headers

    # Drop completely empty rows
    df = df.dropna(how="all").reset_index(drop=True)

    # Drop trailer/summary rows: keep only rows with a loan id
    if "SELLER Loan #" not in df.columns:
        raise KeyError(
            f"'{excel_path}' did not produce a 'SELLER Loan #' column after header detection. "
            f"Detected headers: {list(df.columns)}"
        )
    df = df[df["SELLER Loan #"].notna()].copy()

    # Normalize platform indicator (will be snake_cased later)
    df["Platform"] = platform
    return df


def _normalize_colname(df: pd.DataFrame, preferred: str, candidates: Tuple[str, ...]) -> str:
    """
    Return the existing column name that matches preferred/candidates case-insensitively.
    """
    cols_lower = {c.lower(): c for c in df.columns}
    if preferred.lower() in cols_lower:
        return cols_lower[preferred.lower()]
    for cand in candidates:
        if cand.lower() in cols_lower:
            return cols_lower[cand.lower()]
    raise KeyError(f"Missing required column. Tried: {preferred} / {candidates}. Found: {list(df.columns)}")


def _apply_notes_suffix(df: pd.DataFrame) -> pd.DataFrame:
    """
    Notebook behavior: if Application Type == 'HD NOTE', append 'notes' to Loan Program.
    """
    app_type_col = _normalize_colname(df, "Application Type", ("application_type",))
    program_col = _normalize_colname(
        df,
        "Loan Program",
        ("loan program", "Loan Program Name", "Program", "loan_program"),
    )

    df[program_col] = df[program_col].astype("string")
    mask = df[app_type_col].astype("string").str.upper() == "HD NOTE"
    df.loc[mask, program_col] = df.loc[mask, program_col] + "notes"
    return df


def _load_master_combined(master_path: str, notes_path: str) -> pd.DataFrame:
    """
    Load MASTER_SHEET.xlsx and MASTER_SHEET - Notes.xlsx.
    Notes sheet loan programs get 'notes' suffix to align with Exhibit A normalization.
    """
    master = pd.read_excel(master_path)
    notes = pd.read_excel(notes_path)

    master_prog = _normalize_colname(master, "loan program", ("Loan Program", "Loan Program Name", "Program"))
    notes_prog = _normalize_colname(notes, "loan program", ("Loan Program", "Loan Program Name", "Program"))

    notes[notes_prog] = notes[notes_prog].astype("string") + "notes"
    return pd.concat([master, notes], ignore_index=True)


def _parse_dates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure date fields are real python datetime.date values.
    Critical: Submit Date must be comparable to datetime.date cutoffs in CoMAP.
    """
    if "Submit Date" in df.columns:
        df["Submit Date"] = pd.to_datetime(df["Submit Date"], errors="raise").dt.date

    for col in ["Purchase_Date", "Purchase Date", "Monthly Payment Date"]:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce").dt.date

    return df


def _snake_case_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Convert the canonical fields needed by rules (especially comap.py) into snake_case.
    We only rename the known required columns; everything else is left as-is.
    """
    df = df.rename(
        columns={
            "SELLER Loan #": "seller_loan_no",
            "Platform": "platform",
            "Loan Program": "loan_program",
            "Submit Date": "submit_date",
            "FICO": "fico",
            "FICO Borrower": "fico",
        }
    )

    required = ["seller_loan_no", "platform", "loan_program", "submit_date", "fico"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise KeyError(
            f"build_dataset output is missing required column(s): {missing}. "
            f"Columns present: {list(df.columns)}"
        )

    return df


def build_dataset(
    prime_exhibit_a: str,
    sfy_exhibit_a: str,
    master_sheet: str,
    master_sheet_notes: str,
    output_csv: str,
) -> str:
    prime_df = _clean_exhibit_a(prime_exhibit_a, platform="prime")
    sfy_df = _clean_exhibit_a(sfy_exhibit_a, platform="sfy")

    buy_df = pd.concat([prime_df, sfy_df], ignore_index=True)
    buy_df = _apply_notes_suffix(buy_df)

    master_df = _load_master_combined(master_sheet, master_sheet_notes)

    # Merge keys
    buy_prog = _normalize_colname(
        buy_df,
        "Loan Program",
        ("loan program", "Loan Program Name", "Program", "loan_program"),
    )
    master_prog = _normalize_colname(master_df, "loan program", ("Loan Program", "Loan Program Name", "Program"))

    buy_df["_loan_program_key"] = buy_df[buy_prog].astype("string").str.strip()
    master_df["_loan_program_key"] = master_df[master_prog].astype("string").str.strip()

    # Prefer merge on (Platform, loan program) if Platform exists in master
    if "Platform" in master_df.columns:
        merged = buy_df.merge(master_df, on=["Platform", "_loan_program_key"], how="left", suffixes=("", "_master"))
    else:
        merged = buy_df.merge(master_df, on="_loan_program_key", how="left", suffixes=("", "_master"))

    merged = merged.drop(columns=["_loan_program_key"], errors="ignore")
    merged = _parse_dates(merged)

    # ðŸ”‘ Make output comply with snake_case expectations
    merged = _snake_case_output_columns(merged)

    out_path = Path(output_csv)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    merged.to_csv(out_path, index=False)
    return str(out_path)


def parse_args():
    p = argparse.ArgumentParser(description="Build unified dataset CSV for loan rules engine input.")

    p.add_argument("--prime-exhibit-a", required=True, help="Path to Prime Exhibit A Excel file")
    p.add_argument("--sfy-exhibit-a", required=True, help="Path to SFY Exhibit A Excel file")
    p.add_argument("--master-sheet", required=True, help="Path to MASTER_SHEET.xlsx")
    p.add_argument("--master-sheet-notes", required=True, help="Path to MASTER_SHEET - Notes.xlsx")
    p.add_argument("--output-csv", required=True, help="Output CSV path for run_engine.py input")

    return p.parse_args()


def main():
    args = parse_args()
    _require_venv()

    for pth in [args.prime_exhibit_a, args.sfy_exhibit_a, args.master_sheet, args.master_sheet_notes]:
        if not Path(pth).exists():
            raise FileNotFoundError(f"Input file not found: {pth}")

    out_csv = build_dataset(
        prime_exhibit_a=args.prime_exhibit_a,
        sfy_exhibit_a=args.sfy_exhibit_a,
        master_sheet=args.master_sheet,
        master_sheet_notes=args.master_sheet_notes,
        output_csv=args.output_csv,
    )

    print(f"Built dataset CSV: {out_csv}")


if __name__ == "__main__":
    main()
